---
title: "Statistical Learning, Homework-01"
author: "Barba Paolo, Candi Matteo, Costantini Silvia, Vestini Maria Vittoria"
date: '2023-05-10'



output: 
  html_document:
    code_folding: hide
    theme: 
      color-contrast-warnings: false
      bg: "#2B3E50"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      secondary: "#00DAC6"
      base_font:
      google: Prompt
      heading_font:
        google: Proza Libre
---

```{r setup, include=FALSE}
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```

```{r package, message=FALSE}

rm(list = ls())

# Set reproducibility -----------------------------------------------------
seed <- 1234
set.seed(seed) 

# Libraries and Data ---------------------------------------------------------------

pkt <- c('glmnet', 'NMOF', 'parallel', 'snow', 'ggplot2', 'reshape2', 'gridExtra','tidyverse','manipulate')

invisible(lapply(pkt, require, character.only = TRUE))

```

## Purpose & Statistical tools used

The homework's $1$ purpose is to try to predictic a function $f(\underline{X_i}) = \underline{X_{i}} + \epsilon_i$ [TODO COSa rappresenta sta funzione], by performing a spline regression. 

Fort the Homework's 1 sake we are going to use penalized linear model as linear regression, cross-validation tecnique and in order to implement splines we will use truncated power basis functions.

[TODO] Sistemare

## Be linear in transformed feature space

The basic idea of beeing linear in a transformed feauture space is to map our parameters into a higher space (d - dimension ) and the model will be linear in that space.

The problem that slines are trying to solve is that , differently from polynomial models, where the function are defined globally ,they try to capture local structure of a taget function.


In order to implent splines we use truncated power functions

$$\mathcal{G}_{d,q} = {g_1(x) \dots g_{d+1}(x),g_{(d+1)+1}(x),\dots,g_{(d+1)+q}(x)}$$
defined as:

$\{g_1(x)=1,g_2(x)=x,...,g_{d+1}(x)=x^{d}\}$ and $\{g_{(d+1)+j}(x)= (xâˆ’\xi_{j})^d_+\}$ where $(x)_+ =max\{0,x\}$.


It is easy to show that the spline $f(\cdot)$ can be rappresented by a linear combination of power functions. 

$$f(x)= \sum_{j= 1}^{d+q+1} Î²_j Â·g_j(x)$$
We will estimate the $\beta_s$ coefficiente by using a penalized linear model.



[TODO]  formalizzare, scrivere meglio che fa cagare

## Implementation of the truncated power basis

First of all, we have develop our own implementation of the truncated power basis 
$\mathcal{G}_{d,q}$ with, given a degree $d$ , the number of knots $q$, and their positions, will be able to return a feature matrix $\mathbb{X}$ that will be used in order to get a prediction of the response varible $Y$.

For the sake on our explanation, we will show below few results of the function, by picking the degree $d \in \{1,3\}$ and the $q \in {3,10\}$ equispaced knots.


[TODO]  clean and clever explanations
```{r power_function}

# Function used to compute the feature matrix
power_functions <- function(d, q, knots, x){
  X <- matrix(NA, length(x), d+q+1)  # Pre-set the feature matrix 
  for( i in 1:length(x)){            # Loop over the data-points
    
    for( j in 1:(d+q+1) ){           # Loop over the basis used
      
      if ( j <= d+1 ){               # Check if the index belong to the firsts (d+1) indices  
        X[i,j] <- x[i]^(j-1)         # Compute the powers
      }
      
      else                           # The index do not belong to the firsts (d+1) indices  
        if((x[i] - knots[j-(d+1)])^d > 0){      # Check if the power of the difference between the data-point and the knot is positive
          X[i,j] <- (x[i] - knots[j-(d+1)])^d   # Compute the power of the difference
        }
      else                           # the power is not positive
         X[i,j] <- 0                 # Set the value to 0
    }   
  }
  return(X)                         # Return the feature matrix
}

```



```{r plot truncated power function, fig.showtext = TRUE, fig.height = 7, fig.width = 12, fig.align = "center", message=FALSE, warning=FALSE}

plot_func <- function(d, q){
  knots <- seq(0, 1, length.out = q+2)[2 : (q+1)]
  df <- data.frame(power_functions(d, q, knots, xx))
  n_col <- ncol(df)
  legend_labels <- c()
  for (i in 1:n_col) legend_labels <- c(legend_labels, paste0('g', i, '(x)'))
  colnames(df) <- legend_labels
  df <- cbind('xx' = xx, df)
  df <- melt(df,  id.vars = 'xx')
  
  graph <- ggplot(data=df, aes(x = xx, y = value, color = variable)) + 
    geom_line(linewidth = 1) + 
    ggtitle(paste0('Plot with d = ', d, ' and  q = ', q)) + 
    xlab('x') +
    ylab('y') +
    theme_light() + 
    theme(panel.grid = element_blank(),
          panel.background = element_rect(fill = NA),
          
          title = element_text(colour = "white"),
          
          plot.background = element_rect(fill = NA, colour = "#2B3E50"), 
          plot.title = element_text(size=16),
          
          axis.line = element_line(color = "white"),
          axis.text = element_text(color = "white", size = 10),
          axis.title.y = element_text(angle = 0, vjust = 0.5),
          
          legend.title = element_blank(),
          legend.text = element_text(size = 10, color = 'white'),
          legend.key = element_rect(color = NA, fill = NA),
          legend.key.size = unit(0.6, "cm"),
          legend.background = element_rect(fill = NA))
      
  return(graph)
}

xx <- seq(0, 1, length.out = 1000)

p1 <- plot_func(1, 3)
p2 <- plot_func(3, 3)
p3 <- plot_func(1, 10)
p4 <- plot_func(3, 10)

grid.arrange(p1, p2, p3, p4, ncol=2)

```


### Comments

Untill $j \le d +1$ the power function will be just the value $x^{j-1}$, as soon as $j > d +1$, the power function will be $0$ if the data-point in lower than the knot, otherwise, it will be $(xâˆ’\xi_{j})^d$.

The power basis allow us to have a higher dimension feature space fixing without having a high degree of the used polynomial.

Matrice singolare [TODO non mi ricordo bene cosa significa e cosa consegue ma si vedrÃ ]

[TODO] CLean and Clever



## ChatGPT - Power function

In this section we ask CHATgbt to implement his own version of truncated power basis.
Below are reported the conversation we had with it.

**1Â° Request:**

"Can you implement a function in R to compute the truncated power basis? The parameters of this function are: $d$, $q$, knots. Where $g_i=x^{i-1}$ if $i\le(d+1)$ else $g_{d+1+j} = \max((x-knots[j])^d, 0)$ for $j \in [1,q]$?"

**1Â° Response:**

" Certainly! Here's a function in R that computes the truncated power basis you described:

```{r GPT, class.source = "fold-show"}

truncated_power_basis <- function(x, d, q, knots) {
  
  # Compute the first d+1 basis functions
  basis <- matrix(0, nrow = length(x), ncol = d+1)
  basis[,1] <- 1
  for (i in 2:(d+1)) {
    basis[,i] <- x^(i-1)
  }
  
  # Compute the remaining basis functions using knots
  for (j in 1:q) {
    basis[,d+1+j] <- pmax((x - knots[j])^d, 0)
  }
  
  return(basis)
}
```


This function takes as input the vector x, the integer d, the integer q, and the vector knots. It returns a matrix with length(x) rows and d+1+q columns, where each column represents a basis function."


We have given a try for it!

```{r GPtest, error=TRUE, class.source = "fold-show"}

d <- 1
q <- 3
knots <- seq(0, 1, length.out=q)
x <- seq(0, 1, length.out=1000)
M <- truncated_power_basis(x, d, q, knots)

```

Opsie Dupsy! Looks like the ChatBot response doesn't work. So we will try and see how much we have tell it in order to obtain a working function, first thing to do is give it the error we got. 

*For this following part the code will not be directly shown.*

** 2Â° request: **

"It seems like there is an error with your function, if i try to use it I get this error: "Error in `[<-`(`*tmp*`, , d + 1 + j, value = pmax((x - knots[j])^d, 0)): subscript out of bounds". Could you give me the correct implementation?"

**2Â° response:**

" I apologize for the error in my previous implementation. Here's an updated version of the truncated_power_basis function that should work correctly:

```{r GPtest2, error=TRUE}

truncated_power_basis <- function(x, d, q, knots) {
  # Compute the first d+1 basis functions
  basis <- matrix(0, nrow = length(x), ncol = d+1)
  basis[,1] <- 1
  for (i in 2:(d+1)) {
    basis[,i] <- x^(i-1)
  }
  
  # Compute the remaining basis functions using knots
  if (q > 0) {
    for (j in 1:q) {
      basis[,d+j+1] <- pmax((x - knots[j])^d, 0)
    }
  }
  
  return(basis)
}

```

$\dots$" 

Again it doesn't work, so this time we give the ChatBot a hint, since we have noticed that there isn't enough allocated space for the matrix, this should have $q+d+1$ columns instead of $d+1$.

**3Â° message:**

"It doesn't solve it, I think the problem is in the dimension of the matrix." ðŸ˜‰

**3Â° response:**

"
I apologize for the continued issue. You are correct that there is an issue with the dimension of the matrix. Here is the corrected implementation of the truncated_power_basis function:
```{r GPtest3, error=TRUE}

truncated_power_basis <- function(x, d, q, knots) {
  # Compute the first d+1 basis functions
  basis <- matrix(0, nrow = length(x), ncol = d+1)
  basis[,1] <- 1
  for (i in 2:(d+1)) {
    basis[,i] <- x^(i-1)
  }
  
  # Compute the remaining basis functions using knots
  if (q > 0) {
    for (j in 1:q) {
      basis[,d+j+1] <- pmax((x - knots[j])^(d*(x >= knots[j])), 0)
    }
  }
  
  return(basis)
}
```

..."

At the end since also this implementation doesn't work we decided to tell the ChatBot how to solve the issue

**4Â° message:**

"Still not good, shouldn't the matrix have $q+d+1$ columns since we are trying to evaluate $q+d+1$ functions?" 

**4Â° response:**

"
You are correct. I apologize for the error. The matrix should have $q + d + 1$ columns since we are computing a total of $q + d + 1$ basis functions. Here is the corrected implementation:
```{r GPtest4, error=TRUE}
truncated_power_basis <- function(x, d, q, knots) {
  # Compute the first d+1 basis functions
  basis <- matrix(0, nrow = length(x), ncol = d+1)
  basis[,1] <- 1
  for (i in 2:(d+1)) {
    basis[,i] <- x^(i-1)
  }
  
  # Compute the remaining basis functions using knots
  if (q > 0) {
    for (j in 1:q) {
      basis[,d+j+1] <- pmax((x - knots[j])^(d*(x >= knots[j])), 0)
    }
  }
  
  return(basis)
}

```

..." 

But again even if we straight up told the Bot what's the problem and how to solve it the function is not working! At the end the only thing left to do is just tell it the code it should use.

**5Â° (and final) message:** 
"You are not allocating the memory for the matrix correctly, basis should be defined as "basis <- matrix(0, nrow = length(x), ncol = d+q+1)""

**5Â° response:**

"You are correct again. I apologize for the confusion. Here is the corrected implementation:

```{r GPtest5, error=TRUE, class.source = "fold-show"}
truncated_power_basis <- function(x, d, q, knots) {
  # Compute the first d+1 basis functions
  basis <- matrix(0, nrow = length(x), ncol = d+q+1)
  basis[,1] <- 1
  for (i in 2:(d+1)) {
    basis[,i] <- x^(i-1)
  }
  
  # Compute the remaining basis functions using knots
  if (q > 0) {
    for (j in 1:q) {
      basis[,d+j+1] <- pmax((x - knots[j])^(d*(x >= knots[j])), 0)
    }
  }
  ?pmax
  return(basis)
}
```

$\dots$"

Finally we obtain a function without errors!

```{r GPtest final, error=TRUE, class.source = "fold-show"}
truncated_power_basis(seq(0,1,length.out=6), 3, 2, c(0.1, 0.5))
```

Sadly though we realize that from the third response on the ChatBot changed a line of code from $basis[,d+j+1] <- pmax((x - knots[j])^d, 0)$ to $basis[,d+j+1] <- pmax((x - knots[j])^{d*(x >= knots[j])}, 0)$ and now the function indeed does gives an output, but it's not the right one.



### Comments

The chat we had with ChatGPT was an experience. 

# TODO Finisci di scrivere evaluation

## Dataset

[TODO]

```{r dataset}

test_set_vero <- read.csv("test.csv")
train_set <- read.csv("train.csv")

```

## Vanilla Cross Validation

```{r functions VCV}

# Function to estimate the weights
compute_weights <- function(knots , dataset){
  # Number of knots
  n <- length(knots)
  # Add the bounds 
  knots <- c(0, knots)
  if(knots[n+1] != 1){
    knots <- c(knots, 1)
  }
   # Define the matrix
  xx <- rep(NA, length(dataset$x))
  
   # For each interval bounded by the knots
  for(i in 1:(n+1)){
    # Check if there are at least 2 data points inside the interval
    if(sum(dataset$x >= knots[i] & dataset$x <= knots[i+1])>1){    
      # Compute the conditional variance 
      v  <- var(dataset$y[(dataset$x >= knots[i]) & (dataset$x <= knots[i+1])]) 
      # Set the weight of the data points equal to the inverse of the variance
      xx[dataset$x >= knots[i] & dataset$x <= knots[i+1]] <- 1 / v              
    } 
    # If there is only one data point
    else if(sum(dataset$x >= knots[i] & dataset$x <= knots[i+1]) == 1){  
      # Since we can't compute the variance of a single data point then we use the previously computed variance
      xx[dataset$x >= knots[i] & dataset$x <= knots[i+1]] <- 1/v
    }
  }
  
  return(xx)
}


# Function used for the cross validation
cross_val_func <- function(x){
  set.seed(070720)
  # Define the parameters
  d <- x[1]   
  q <- x[2]
  k <- x[3]
  a <- x[4]
  l <- x[5] 
  p <- x[6]
  
  # size of the fold
  l_folds <- nrow(train_set) / k 
  # Pre-set the scores (MSE)
  score <- rep(NA, k)
  # Random  sample of the indices
  idx <- sample((1:nrow(train_set)),nrow(train_set))
  #Loop over the folds
  for ( i in 1:k){
    # Set the validation and train set
    cv_test <- train_set[idx[((i-1)*l_folds+1): (i*l_folds)],]
    cv_train <- train_set[-idx[((i-1)*l_folds+1): (i*l_folds)],]
    
    # Conpute the knots
    knots <- seq(1/q, p, length.out=q)
    
    # Compute the power function on the training set
    M_cv_train <- power_functions(d = d, q = q, knots = knots, x = cv_train$x)
    
    # Compute the power function on the valifation set
    M_cv_test <-  power_functions(d = d, q = q, knots = knots, x = cv_test$x)

    # compute the weights, based on the conditional variance
    hat_weights <- compute_weights(knots = knots , cv_train)
    
    # Train the model
    cv_model <- glmnet(M_cv_train, 
                       cv_train$y,
                       family = "gaussian", 
                       alpha=a, 
                       lambda=l,
                       weights = hat_weights)
    
    # Preditcion on the validation set
    cv_predictions <- predict(cv_model, M_cv_test)
    
    # Compute the score
    score[i] <- sqrt(mean((cv_test$y-cv_predictions)^2))
    
  }
  # Return the mean MSE  
  return(mean(score))
}
```

```{r parametes}
k <- c(4)
d_grid <- c(1, 3) 
q_grid <- seq(3, 30, 1)
positions <- c(0.5, 0.6, 0.7, 0.8)
lambdas <- 10^seq(-2.5, -1, .25)
alphas <- seq(0, 1, 0.1)
# Set the parameter for the CV
parameters <- list(d_grid, q_grid, k, alphas, lambdas, positions)


```

```{r cv vanilla, eval = FALSE}
# CV vanilla --------------------------------------------------------------
# Select the best combination of parameters
cl = makeCluster(detectCores())
clusterExport(cl, c('train_set', 'power_functions', 'glmnet', 'compute_weights'))
res <- gridSearch(cross_val_func, levels=parameters, method = 'snow', cl=cl)
stopCluster(cl)
best_params <- res$minlevels
names(best_params) <- c('d', 'q', 'k', 'alpha', 'lambda', 'position')
```


```{r training with vanilla parameters, warning = FALSE}
load("RData/best_params_vanilla.RData")
# Prediction --------------------------------------------------------------

# Using the best parameters
d_best <- best_params[1]
q_best <- best_params[2]
k_best <- best_params[3]
a_best <- best_params[4]
l_best <- best_params[5]
p_best <- best_params[6]

# Compute the predictions
knots <- seq(1/q_best, p_best, length.out=q_best)
M_train <- power_functions(d = d_best, q = q_best, knots = knots, x = train_set$x)
M_test <- power_functions( d = d_best, q = q_best, knots = knots, x = test_set_vero$x)
knots_test <- power_functions( d = d_best, q = q_best, knots = knots, x = knots)
hat_weights <- compute_weights(knots , train_set)
final_model <- glmnet(M_train, train_set$y, family ="gaussian", 
                      alpha=a_best, lambda=l_best , weights = hat_weights )
predictions <- predict(final_model,M_test)
best_params
```


```{r plot after vanilla, fig.showtext = TRUE, fig.height = 5, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE}
colors <- c("Real Data" = "gold", "Predicted" = "white", "Knots" = "red")
point_size <- 1.7
cross_size <- 3

ggplot() +
  geom_point(aes(x = train_set$x, y = train_set$y, color = 'Real Data'), size = point_size, shape=16) +
  geom_point(aes(x = test_set_vero$x, y = predictions, color = 'Predicted'), size = point_size) +
  geom_point(aes(x = knots, y = predict(final_model, knots_test), color = 'Knots'), shape = 4, stroke = 1.7, size = cross_size) +
  theme_light() +
  labs(x = "x", y = "y", color = "     Legend", title = 'Prediction on WMAP data', shape = "", color="") +
  scale_color_manual(values = colors) +
  guides(color = guide_legend(override.aes=list(shape = c(4, 16, 16), size = 2))) +
  theme(legend.title = element_text(size=12, color='white'),
        plot.title = element_text(hjust = 0.5,size=16),
        legend.text = element_text(size = 10, color = 'white'),
        legend.key = element_rect(color = NA, fill = NA),
        legend.background = element_rect(fill = NA),
        panel.grid.major = element_line(size = 0.3, linetype = 2),
        panel.grid.minor = element_line(size = 0, linetype = 2),
        panel.background = element_rect(fill = NA),
        title = element_text(colour = "white"),
        plot.background = element_rect(fill = NA, colour = "#2B3E50"), 
        axis.line = element_line(color = "white"),
        axis.text = element_text(color = "white", size = 10),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```

## Update the parameter


```{r update parameter , warning=FALSE}
# Update Parameters -------------------------------------------------------------

# Using the best parameters
d_best <- best_params[1]
q_best <- best_params[2]
k_best <- best_params[3]
a_best <- best_params[4]
l_best <- best_params[5]
p_best <- best_params[6]

d <- d_best
q <- q_best+seq(-2,2,1)
k <- k_best
a <- a_best
l <- l_best + seq(-0.005,0.005,0.001)
p <- p_best + seq(-0.05, 0.05, 0.025)

# Set the parameter for the CV
parameters <- list(d, q, k, a, l, p)

```


## Nested Cross Validation

```{r nested cross validation}

# Secondary function for nested CV
inner_crossval <- function(x, train_set){
  # define the parameters
  d <- x[1]
  q <- x[2]
  K <- x[3]
  a <- x[4]
  l <- x[5] 
  p <- x[6]
  
  # Pre-set the error
  e_in <- c()
  # Loop over k-1 fold
  for(k in (1:(K-1))){
    # Set the indices
    idx <- ((k-1)*l_folds+1): (k*l_folds)
    
    # Validation set
    cv_test <- train_set[idx,]
    # Train set
    cv_train <- train_set[-idx,]
    
    # Define the knots
    knots <- seq(1/q, p, length.out=q)
    
    
    #Computhe the power functions
    M_cv_train <- power_functions(d = d, q = q, knots = knots, x = cv_train$x)
    M_cv_test <-  power_functions(d = d, q = q, knots = knots, x = cv_test$x)
    
    # Computhe the weights based on the conditional variance
    hat_weights <- compute_weights(knots = knots , cv_train)
      
    # Train the model
    cv_model <- glmnet(M_cv_train, 
                       cv_train$y,
                       family = "gaussian", 
                       alpha=a, 
                       lambda=l,
                       weights = hat_weights)
    
    # prediction
    cv_predictions <- predict(cv_model, M_cv_test)
    
    # Compute the error
    e_temp <- (cv_test$y-cv_predictions)^2
    e_in <- c(e_in, e_temp)
  }
  
  return(e_in)
}

# Main function for the nested CV
nested_crossval <- function(x){
  # set the parameters
  d <- x[1]
  q <- x[2]
  K <- x[3]
  a <- x[4]
  l <- x[5] 
  p <- x[6]
  R <- 250
  
  # Define the length of the folds
  l_folds <<- nrow(train_set) / K
  
  #
  a_list <- rep(NA, R*K)
  #
  b_list <- rep(NA, R*K)
  
  for(r in (1:R)){
    idx <- sample((1:nrow(train_set)),nrow(train_set))
    
    for(k in (1:K)){  
      cv_test <- train_set[idx[((k-1)*l_folds+1): (k*l_folds)],]
      cv_train <- train_set[-idx[((k-1)*l_folds+1): (k*l_folds)],]
      
      # inner cross
      e_in <- inner_crossval(x, cv_train)
      
      # Outer cross
      knots <- seq(1/q, p, length.out=q)
     
      M_cv_train <- power_functions(d = d, q = q, knots = knots, x = cv_train$x)
      M_cv_test <-  power_functions(d = d, q = q, knots = knots, x = cv_test$x)
      
      hat_weights <- compute_weights(knots = knots , cv_train)
      
      cv_model <- glmnet(M_cv_train, 
                         cv_train$y,
                         family = "gaussian", 
                         alpha=a, 
                         lambda=l,
                         weights = hat_weights)
      
      cv_predictions <- predict(cv_model, M_cv_test)
      
      e_out <- (cv_test$y-cv_predictions)^2
      
      a_list[(r-1)*K+k] <- (mean(e_in)-mean(e_out))^2 
      b_list[(r-1)*K+k] <- (sd(e_out)^2)/l_folds
      #es <- c(es, e_in)   
    }
  }
  
  mse <- mean(a_list)-mean(b_list)
  #err <- mean(es)
  return(mse)
}

```


```{r run nested cv , eval = FALSE}
# CV nested --------------------------------------------------------------
cl = makeCluster(detectCores())
clusterExport(cl, c('train_set','compute_weights' ,'inner_crossval', 'power_functions', 'glmnet'))
res <- gridSearch(nested_crossval, levels=parameters, method = 'snow', cl=cl)
stopCluster(cl)
best_params <- res$minlevels
names(best_params) <- c('d', 'q', 'k', 'alpha', 'lambda','position')
```


```{r training with nested parameters, warning = FALSE}

# Prediction --------------------------------------------------------------

# Using the best parameters
d_best <- best_params[1]
q_best <- best_params[2]
k_best <- best_params[3]
a_best <- best_params[4]
l_best <- best_params[5]
p_best <- best_params[6]

# Compute the predictions
knots <- seq(1/q_best, p_best, length.out=q_best)
M_train <- power_functions(d = d_best, q = q_best, knots = knots, x = train_set$x)
M_test <- power_functions( d = d_best, q = q_best, knots = knots, x = test_set_vero$x)
knots_test <- power_functions( d = d_best, q = q_best, knots = knots, x = knots)
hat_weights <- compute_weights(knots , train_set)
final_model <- glmnet(M_train, train_set$y, family ="gaussian", 
                      alpha=a_best, lambda=l_best , weights = hat_weights )
predictions <- predict(final_model, M_test)
best_params
```



```{r plot after nested, fig.showtext = TRUE, fig.height = 5, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE}


colors <- c("Real Data" = "gold", "Predicted" = "white", "Knots" = "red")
point_size <- 1.7
cross_size <- 3

ggplot() +
  geom_point(aes(x = train_set$x, y = train_set$y, color = 'Real Data'), size = point_size, shape=16) +
  geom_point(aes(x = test_set_vero$x, y = predictions, color = 'Predicted'), size = point_size) +
  geom_point(aes(x = knots, y = predict(final_model, knots_test), color = 'Knots'), shape = 4, stroke = 1.7, size = cross_size) +
  theme_light() +
  labs(x = "x", y = "y", color = "     Legend", title = 'Prediction on WMAP data', shape = "", color="") +
  scale_color_manual(values = colors) +
  guides(color = guide_legend(override.aes=list(shape = c(4, 16, 16), size = 2))) +
  theme(legend.title = element_text(size=12, color='white'),
        plot.title = element_text(hjust = 0.5,size=16),
        legend.text = element_text(size = 10, color = 'white'),
        legend.key = element_rect(color = NA, fill = NA),
        legend.background = element_rect(fill = NA),
        panel.grid.major = element_line(size = 0.3, linetype = 2),
        panel.grid.minor = element_line(size = 0, linetype = 2),
        panel.background = element_rect(fill = NA),
        title = element_text(colour = "white"),
        plot.background = element_rect(fill = NA, colour = "#2B3E50"), 
        axis.line = element_line(color = "white"),
        axis.text = element_text(color = "white", size = 10),
        axis.title.y = element_text(angle = 0, vjust = 0.5))
```



# Conclusion & Final Remark
